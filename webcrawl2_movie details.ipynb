{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2642c6d5",
   "metadata": {},
   "source": [
    "# Webcrawling process (cont')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2cdee44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dabd49",
   "metadata": {},
   "source": [
    "## Get Movie Details of each film\n",
    "This is the hardest part, not only does it takes time but there is also a risk of being temporarily/permanently blocked by the site (Error 403 Forbidden)..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c9081c",
   "metadata": {},
   "source": [
    "### List of browers to rotate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f95d69a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADERS_LIST = [\n",
    "  {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0\",\n",
    "    \"X-Requested-With\": \"XMLHttpRequest\"\n",
    "  },\n",
    "  {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.13; rv:110.0) Gecko/20100101 Firefox/110.0\",\n",
    "    \"X-Requested-With\": \"XMLHttpRequest\"\n",
    "  },\n",
    "  {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.75 Safari/537.36\",\n",
    "    \"X-Requested-With\": \"XMLHttpRequest\"\n",
    "  },\n",
    "  {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36 Edg/110.0.1587.50\",\n",
    "    \"X-Requested-With\": \"XMLHttpRequest\"\n",
    "  },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e92df97",
   "metadata": {},
   "source": [
    "### Old: Parallel Webscraping - 20s/movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e435f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "def parse_movie_details(soup, url):\n",
    "  \"\"\"\n",
    "  Shared parsing logic for movie details from BeautifulSoup object.\n",
    "  Returns a dictionary with the structured data.\n",
    "  \"\"\"\n",
    "  # Find the Movie Details section\n",
    "  movie_details = {'link': url}\n",
    "  \n",
    "  # Get table #1 (index 1) - the metrics table\n",
    "  production_budget = ''\n",
    "  all_tables = soup.find_all('table', limit=4)\n",
    "  metrics_table = all_tables[1]\n",
    "  \n",
    "  # Search all rows in table #1 for \"Production Budget\"\n",
    "  rows = metrics_table.find_all('tr')\n",
    "  for row in rows:\n",
    "    cells = row.find_all(['td', 'th'])\n",
    "    for cell in (cells):\n",
    "      text = cell.get_text(strip=True)\n",
    "      # Check if this cell contains \"Production Budget\"\n",
    "      if 'production budget' in text.lower():\n",
    "        production_budget = re.search(r'\\$?([\\d,]+)(?![\\d,])', text).group(0)\n",
    "        movie_details['Production Budget'] = production_budget\n",
    "  \n",
    "  # Look for the table with Movie Details\n",
    "  details_table = all_tables[3]\n",
    "  \n",
    "  if details_table:\n",
    "    rows = details_table.find_all('tr')\n",
    "    for row in rows:\n",
    "      cells = row.find_all(['td', 'th'])\n",
    "      if len(cells) >= 2:\n",
    "          key = cells[0].get_text(strip=True).replace('\\xa0', ' ')\n",
    "          value = cells[1].get_text(strip=True).replace('\\xa0', ' ')\n",
    "          \n",
    "          # Clean up the key (remove colons and extra spaces)\n",
    "          key = key.replace(':', '').strip()\n",
    "          \n",
    "          # Skip unwanted fields completely\n",
    "          if key in ['Video Release', 'Comparisons', 'Keywords', 'Source', 'Languages']:\n",
    "              continue\n",
    "          \n",
    "          # MPAA Rating:\n",
    "          if key == 'MPAA Rating':\n",
    "            allowed_ratings = ['PG-13', 'NC-17', 'PG', 'R', 'G']  # Order matters: check longer ones first\n",
    "            upper_value = value.upper()\n",
    "            found_rating = None\n",
    "            for rating in allowed_ratings:\n",
    "              if rating in upper_value:\n",
    "                found_rating = rating\n",
    "                break\n",
    "            movie_details['MPAA Rating'] = found_rating\n",
    "\n",
    "          # Handle Production Countries and Languages separately\n",
    "          if key == 'Production Countries':\n",
    "              # Check if Languages data is mixed in\n",
    "              if 'Languages:' in value:\n",
    "                  parts = value.split('Languages:')\n",
    "                  movie_details['Production Countries'] = parts[0].strip()\n",
    "                  if len(parts) > 1:\n",
    "                      movie_details['Languages'] = parts[1].strip()\n",
    "              else:\n",
    "                  movie_details['Production Countries'] = value\n",
    "          elif key == 'Languages':\n",
    "              movie_details['Languages'] = value\n",
    "          else:\n",
    "              # Store all other fields\n",
    "              movie_details[key] = value\n",
    "  \n",
    "  # Extract earliest release date from Domestic and International releases\n",
    "  release_dates = []\n",
    "  \n",
    "  # Extract dates from Domestic Releases\n",
    "  if 'Domestic Releases' in movie_details:\n",
    "      domestic_text = movie_details['Domestic Releases']\n",
    "      # Look for date patterns like \"February 14th, 2025\"\n",
    "      domestic_dates = re.findall(r'([A-Za-z]+ \\d{1,2}(?:st|nd|rd|th)?, \\d{4})', domestic_text)\n",
    "      release_dates.extend(domestic_dates)\n",
    "  \n",
    "  # Extract dates from International Releases\n",
    "  if 'International Releases' in movie_details:\n",
    "      intl_text = movie_details['International Releases']\n",
    "      # Look for date patterns like \"January 29th, 2025\"\n",
    "      intl_dates = re.findall(r'([A-Za-z]+ \\d{1,2}(?:st|nd|rd|th)?, \\d{4})', intl_text)\n",
    "      release_dates.extend(intl_dates)\n",
    "  \n",
    "  # Find the earliest date\n",
    "  if release_dates:\n",
    "      try:\n",
    "          # Convert dates to datetime objects for comparison\n",
    "          parsed_dates = []\n",
    "          for date_str in release_dates:\n",
    "              try:\n",
    "                  # Handle ordinal suffixes (st, nd, rd, th)\n",
    "                  clean_date = re.sub(r'(\\d+)(st|nd|rd|th)', r'\\1', date_str)\n",
    "                  parsed_date = datetime.strptime(clean_date, '%B %d, %Y')\n",
    "                  parsed_dates.append(parsed_date)\n",
    "              except:\n",
    "                  continue\n",
    "          \n",
    "          if parsed_dates:\n",
    "              earliest_date = min(parsed_dates)\n",
    "              movie_details['Release Date'] = earliest_date.strftime('%B %d, %Y')\n",
    "      except:\n",
    "          pass\n",
    "  \n",
    "  # Remove the original release fields since we now have Release Date\n",
    "  movie_details.pop('Domestic Releases', None)\n",
    "  movie_details.pop('International Releases', None)\n",
    "  \n",
    "  return movie_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c993c4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_movie_details(url):\n",
    "  \"\"\"\n",
    "  Scrape the Movie Details section from the-numbers.com\n",
    "  Returns a dictionary with the structured data\n",
    "  \"\"\"\n",
    "  header = random.choice(HEADERS_LIST)  # Use choice() not choices()\n",
    "  \n",
    "  s = requests.Session()\n",
    "  r = s.get(url, headers=header, timeout=15)\n",
    "  soup = BeautifulSoup(r.text, 'html.parser')\n",
    "  \n",
    "  # Use the shared parsing logic\n",
    "  return parse_movie_details(soup, url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5256439f",
   "metadata": {},
   "source": [
    "### New: Async webscraping - 1s/movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a0b1ecf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm.asyncio import tqdm\n",
    "\n",
    "max_concurrency = 10\n",
    "sem = asyncio.Semaphore(max_concurrency)\n",
    "timeout_urls = []\n",
    "\n",
    "async def scrape_movie_details_async(session, header, url):\n",
    "  \"\"\"\n",
    "  Async scraping of movie details.\n",
    "  \"\"\"\n",
    "  async with sem:\n",
    "    try:\n",
    "      async with session.get(url, headers=header, timeout=25) as r:\n",
    "        status = r.status\n",
    "        if status != 200:\n",
    "          print(f\"HTTP {status} for {url}\")\n",
    "          return None\n",
    "        html = await r.text()\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        \n",
    "        # Use the shared parsing logic\n",
    "        data = parse_movie_details(soup, url)\n",
    "        # Treat dicts with only 'link' as invalid\n",
    "        if isinstance(data, dict) and len(data) <= 1:\n",
    "          print(f\"Parsed no data for {url}\")\n",
    "          return None\n",
    "        return data\n",
    "    \n",
    "    except asyncio.TimeoutError:\n",
    "        print(f\"Timeout error for {url}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error for {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "async def scrape_batch(session, header, urls):\n",
    "  \"\"\"Async scraping, continued - callable function.\"\"\"\n",
    "  if session is None:\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "      tasks = [scrape_movie_details_async(session, header, url) for url in urls]\n",
    "      return await tqdm.gather(*tasks)\n",
    "  else:\n",
    "    tasks = [scrape_movie_details_async(session, header, url) for url in urls]\n",
    "    return await tqdm.gather(*tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f392eff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def process_batch_with_error_handling(session, header, urls, batch_num):\n",
    "  \"\"\"Process one batch with error handling\"\"\"\n",
    "  try:\n",
    "    print(f\"========== PROCESSING BATCH {batch_num:02d} ({len(urls)} URLs)... ==========\")\n",
    "\n",
    "    results = await scrape_batch(session, header, urls)\n",
    "    \n",
    "    # Keep only non-empty dicts with more than just the link\n",
    "    valid_results = []\n",
    "    for r in results:\n",
    "      if isinstance(r, dict) and len(r) > 1:\n",
    "        valid_results.append(r)\n",
    "    \n",
    "    if valid_results:\n",
    "      # Convert to DataFrame\n",
    "      df = pd.DataFrame(valid_results)\n",
    "      \n",
    "      # Save to CSV\n",
    "      filename = f'Movie Details/movie_details_{batch_num:02d}.csv'\n",
    "      df.to_csv(filename, index=False)\n",
    "      print(f\"‚úÖ Batch {batch_num:02d} completed: {len(valid_results)} movies saved to {filename}\")\n",
    "      return len(valid_results)\n",
    "    else:\n",
    "      print(f\"‚ö†Ô∏è Batch {batch_num:02d} completed but no valid data\")\n",
    "      return None\n",
    "            \n",
    "  except Exception as e:\n",
    "    print(f\"‚ùå Error in batch {batch_num:02d}: {e}\")\n",
    "    return None\n",
    "\n",
    "async def process_all_batches(urls, start_at=0):\n",
    "  \"\"\"Process all batches with error handling.\n",
    "  Accepts:\n",
    "    - string URL ‚Üí one batch with one URL\n",
    "    - list[str]  ‚Üí one batch with many URLs\n",
    "    - list[list[str]] ‚Üí multiple batches (original behavior)\n",
    "  \"\"\"\n",
    "  # Normalize input into list of batches (list[list[str]])\n",
    "  if isinstance(urls, str):\n",
    "    batches = [[urls]]\n",
    "  elif isinstance(urls, list):\n",
    "    if len(urls) == 0:\n",
    "      batches = []\n",
    "    elif all(isinstance(u, str) for u in urls):\n",
    "      batches = [urls]\n",
    "    else:\n",
    "      batches = urls\n",
    "  else:\n",
    "    batches = []\n",
    "  \n",
    "  # Clamp start_at\n",
    "  if start_at is None or not isinstance(start_at, int):\n",
    "    start_at = 0\n",
    "  if start_at < 0:\n",
    "    start_at = 0\n",
    "  if 90 > start_at >= len(batches) > 0:\n",
    "    start_at = len(batches) - 1\n",
    "  if start_at == 90:\n",
    "    start_at = 90\n",
    "  \n",
    "  total_processed = 0\n",
    "\n",
    "  async with aiohttp.ClientSession() as session:\n",
    "    batch_num = start_at\n",
    "    for batch_urls in batches:\n",
    "      batch_num += 1\n",
    "      i = (batch_num - 1) // 2 % len(HEADERS_LIST)\n",
    "      header = HEADERS_LIST[i]\n",
    "      processed_count = await process_batch_with_error_handling(session, header, batch_urls, batch_num)\n",
    "      total_processed += processed_count\n",
    "      \n",
    "      # Small delay between batches to be nice to the server\n",
    "      await asyncio.sleep(1)\n",
    "  \n",
    "  print(f\"\\nüéâ All batches completed! Total movies processed: {total_processed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b560dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_WW_all = pd.read_csv(\"WW_all.csv\")\n",
    "all_movie_details = []\n",
    "links = [df_WW_all['link'].tolist()[x:x+200] for x in range(0, len(df_WW_all), 200)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9f815ba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links_to_scrape = links[24]\n",
    "isinstance(links_to_scrape, list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "674d74b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape_movie_details(links_to_scrape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cf7e85f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== PROCESSING BATCH 99 (40 URLs)... ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|‚ñà‚ñà‚ñà‚ñà      | 16/40 [00:29<00:48,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timeout error for https://www.the-numbers.com/movie/Grand-Prix-of-Europe-(2025-Germany)#tab=summary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 19/40 [00:34<00:34,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timeout error for https://www.the-numbers.com/movie/Chosen-The-Last-Supper-Part-2-(2025)#tab=summary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 21/40 [00:36<00:22,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timeout error for https://www.the-numbers.com/movie/Hi-Five-(2025-South-Korea)#tab=summary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 23/40 [00:45<00:45,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timeout error for https://www.the-numbers.com/movie/Death-of-a-Unicorn-(2025)#tab=summary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 24/40 [00:46<00:35,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timeout error for https://www.the-numbers.com/movie/Presence-(2025)#tab=summary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 25/40 [00:51<00:45,  3.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timeout error for https://www.the-numbers.com/movie/Padre-No-Hay-Mas-Que-Uno-5-(2025-Spain)#tab=summary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 27/40 [00:57<00:38,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timeout error for https://www.the-numbers.com/movie/You-Are-The-Best-(2025-China)#tab=summary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 28/40 [00:59<00:32,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timeout error for https://www.the-numbers.com/movie/Na-derevnyu-dedushke-(2025-Russia)#tab=summary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 29/40 [01:00<00:24,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timeout error for https://www.the-numbers.com/movie/Colorful-Stage-The-Movie-A-Miku-Who-Cant-Sing-(2025-Japan)#tab=summary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 30/40 [01:01<00:18,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timeout error for https://www.the-numbers.com/movie/Chosen-The-Last-Supper-Part-2-(2025)#tab=summary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 31/40 [01:02<00:14,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timeout error for https://www.the-numbers.com/movie/Three-Kingdoms-Starlit-Heroes-(2025-China)#tab=summary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 32/40 [01:08<00:23,  2.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timeout error for https://www.the-numbers.com/movie/Cang-Mang-De-Tian-Ya-Shi-Wo-De-Ai-(2025-China)#tab=summary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 33/40 [01:11<00:20,  2.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timeout error for https://www.the-numbers.com/movie/Io-Sono-La-Fine-Del-Mondo-(2025-Italy)#tab=summary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 34/40 [01:12<00:14,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timeout error for https://www.the-numbers.com/movie/Friendship-(2025)#tab=summary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 35/40 [01:17<00:15,  3.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timeout error for https://www.the-numbers.com/movie/Chosen-The-Last-Supper-Part-3-(2025)#tab=summary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 36/40 [01:20<00:12,  3.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timeout error for https://www.the-numbers.com/movie/Last-Rodeo-The-(2025)#tab=summary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 37/40 [01:23<00:09,  3.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timeout error for https://www.the-numbers.com/movie/Hotline-Beijing-(2025-China)#tab=summary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 38/40 [01:25<00:05,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timeout error for https://www.the-numbers.com/movie/Red-Silk-(2025-Russia)#tab=summary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 39/40 [01:26<00:02,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timeout error for https://www.the-numbers.com/movie/Dracula-A-Love-Tale-(2025-France)#tab=summary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 40/40 [01:27<00:00,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timeout error for https://www.the-numbers.com/movie/Eddington-(2025)#tab=summary\n",
      "‚úÖ Batch 99 completed: 20 movies saved to Movie Details/movie_details_99.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéâ All batches completed! Total movies processed: 20\n"
     ]
    }
   ],
   "source": [
    "await process_all_batches(urls=links_to_scrape, start_at=98)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af606253",
   "metadata": {},
   "source": [
    "## Concatenate to one dataframe & Export to csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b6dd4406",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "def export_all_movie_details():\n",
    "  all_movie_details = [pd.read_csv(file) for file in glob.glob('Movie Details/*.csv')]\n",
    "  global movie_details_df\n",
    "  # Concatenate all series to a dataframe\n",
    "  if all_movie_details:\n",
    "    movie_details_df = pd.concat(all_movie_details, axis=0, ignore_index=True)\n",
    "    move_col = movie_details_df.pop('Release Date')\n",
    "    movie_details_df.insert(1,'Release Date', move_col)\n",
    "    \n",
    "    print(\"DataFrame shape:\", movie_details_df.shape)\n",
    "    print(\"\\nDataFrame columns:\", movie_details_df.columns.tolist())\n",
    "    print(\"\\nDataFrame content:\")\n",
    "  else:\n",
    "    movie_details_df = pd.DataFrame()\n",
    "\n",
    "  movie_details_df.to_csv('movie_details.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a0b8d847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame shape: (5180, 12)\n",
      "\n",
      "DataFrame columns: ['link', 'Release Date', 'Production Budget', 'MPAA Rating', 'Running Time', 'Franchise', 'Genre', 'Production Method', 'Creative Type', 'Production/Financing Companies', 'Production Countries', 'Languages']\n",
      "\n",
      "DataFrame content:\n"
     ]
    }
   ],
   "source": [
    "export_all_movie_details()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47136cd5",
   "metadata": {},
   "source": [
    "### Re-run timeouts & append to final dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7f524f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_WW_all = pd.read_csv(\"WW_all.csv\")\n",
    "movie_details_df = pd.read_csv(\"movie_details.csv\").dropna(subset=['link'])\n",
    "# Find differences\n",
    "\n",
    "async def retry_timeouts(retry_start=91):\n",
    "  timeouts_df = df_WW_all[~df_WW_all['link'].isin(movie_details_df['link'])]\n",
    "  timeout_urls = [timeouts_df['link'].tolist()[x:x+400] for x in range(0, len(timeouts_df), 400)]\n",
    "  if not timeout_urls:\n",
    "    print(\"No more timeouts :D\")\n",
    "  else:\n",
    "    print(f\"Timeouts: {timeouts_df.shape[0]} missing movies.\")\n",
    "    print(f\"Retrying timed out URLs...\")\n",
    "    await process_all_batches(urls=timeout_urls, start_at=retry_start)\n",
    "    export_all_movie_details()\n",
    "    await retry_timeouts(retry_start + 1)\n",
    "\n",
    "await retry_timeouts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd3738c",
   "metadata": {},
   "source": [
    "## Merge to final `df_WW_all`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7870e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 4784 entries, 0 to 4946\n",
      "Data columns (total 18 columns):\n",
      " #   Column                    Non-Null Count  Dtype \n",
      "---  ------                    --------------  ----- \n",
      " 0   link                      4784 non-null   object\n",
      " 1   Year Recorded             4784 non-null   int64 \n",
      " 2   Rank                      4784 non-null   int64 \n",
      " 3   Movie                     4784 non-null   object\n",
      " 4   Worldwide Box Office      4784 non-null   object\n",
      " 5   Domestic Box Office       3462 non-null   object\n",
      " 6   International Box Office  4744 non-null   object\n",
      " 7   Domestic Share            3462 non-null   object\n",
      " 8   Distributor               3300 non-null   object\n",
      " 9   Production Budget         2713 non-null   object\n",
      " 10  Running Time              4476 non-null   object\n",
      " 11  Genre                     4701 non-null   object\n",
      " 12  Production Method         4686 non-null   object\n",
      " 13  Creative Type             4637 non-null   object\n",
      " 14  MPAA Rating               3577 non-null   object\n",
      " 15  Franchise                 1204 non-null   object\n",
      " 16  Production Countries      4762 non-null   object\n",
      " 17  Release Date              4626 non-null   object\n",
      "dtypes: int64(2), object(16)\n",
      "memory usage: 710.1+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "df_WW_all = pd.read_csv(\"WW_all.csv\")\n",
    "movie_details_df = pd.read_csv(\"movie_details.csv\").dropna(subset=['link'])\n",
    "# MERGE\n",
    "df = pd.merge(df_WW_all, movie_details_df, on='link', how='right').sort_values(by=['Year Recorded','Rank'])\n",
    "\n",
    "# CLEAN MPAA RATING - extract only the rating (PG, PG-13, R, G, NC-17)\n",
    "if 'MPAA Rating' in df.columns:\n",
    "    allowed_ratings = ['PG-13', 'NC-17', 'PG', 'R', 'G']  # Order matters: check longer ones first\n",
    "    def extract_rating(x):\n",
    "        if pd.isna(x):\n",
    "            return x\n",
    "        x_str = str(x).upper()\n",
    "        for rating in allowed_ratings:\n",
    "            if rating in x_str:\n",
    "                return rating\n",
    "        return None\n",
    "    df['MPAA Rating'] = df['MPAA Rating'].apply(extract_rating)\n",
    "\n",
    "# COMBINE 2 COLUMNS\n",
    "df[\"Distributor_y\"] = np.where(\n",
    "    df[\"Distributor\"].isna() | (df[\"Distributor\"] == \"\"),  # A2 = \"\"\n",
    "    df[\"Production/Financing Companies\"].apply(\n",
    "        lambda x: x.split(\",\")[0].strip() if isinstance(x, str) and \",\" in x else np.nan\n",
    "    ),\n",
    "    np.nan\n",
    ")\n",
    "df[\"Distributor_y\"] = df[\"Distributor_y\"].fillna(df[\"Distributor\"])\n",
    "df = df.drop(columns=['Distributor'])\n",
    "\n",
    "# RENAME\n",
    "df = df.rename(columns={'Distributor_y': 'Distributor',\n",
    "                      'Genre_y': 'Genre',\n",
    "                      'Release Date_y': 'Release Date'})\n",
    "\n",
    "# REORDER\n",
    "df = df[['link', 'Year Recorded', 'Rank', 'Movie', 'Worldwide Box Office', 'Domestic Box Office', 'International Box Office', 'Domestic Share', 'Distributor', 'Production Budget', 'Running Time', 'Genre', 'Production Method', 'Creative Type', 'MPAA Rating', 'Franchise', 'Production Countries', 'Release Date']].drop_duplicates(subset=['Movie'])\n",
    "df.to_csv('WW_all_new.csv', index=False)\n",
    "\n",
    "df.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
