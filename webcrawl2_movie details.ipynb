{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2642c6d5",
   "metadata": {},
   "source": [
    "# Webcrawling process (cont') - đang bỏ dở, có thể không làm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b2cdee44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import warnings\n",
    "# warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "89b05c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4829 entries, 0 to 4828\n",
      "Data columns (total 13 columns):\n",
      " #   Column                      Non-Null Count  Dtype \n",
      "---  ------                      --------------  ----- \n",
      " 0   Movie                       4829 non-null   object\n",
      " 1   link                        4829 non-null   object\n",
      " 2   Worldwide Box Office        4829 non-null   object\n",
      " 3   Domestic Box Office         3496 non-null   object\n",
      " 4   International Box Office    4786 non-null   object\n",
      " 5   Domestic Share              3496 non-null   object\n",
      " 6   Share Of Number One Market  2868 non-null   object\n",
      " 7   Number One Market           2955 non-null   object\n",
      " 8   Release Date                3003 non-null   object\n",
      " 9   Distributor                 2980 non-null   object\n",
      " 10  Genre                       3004 non-null   object\n",
      " 11  Rank                        4829 non-null   int64 \n",
      " 12  Year Recorded               4829 non-null   int64 \n",
      "dtypes: int64(2), object(11)\n",
      "memory usage: 490.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df_WW_all = pd.read_csv(\"WW_all.csv\")\n",
    "df_WW_all.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dabd49",
   "metadata": {},
   "source": [
    "## Get Movie Details of each film\n",
    "This is the hardest part, not only does it takes time but there is also a risk of being temporarily/permanently blocked by the site (Error 403 Forbidden)..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c9081c",
   "metadata": {},
   "source": [
    "### List of browers to rotate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95d69a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADERS_LIST = [\n",
    "  {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.75 Safari/537.36\",\n",
    "    \"X-Requested-With\": \"XMLHttpRequest\"\n",
    "  },\n",
    "  {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36 Edg/110.0.1587.50\",\n",
    "    \"X-Requested-With\": \"XMLHttpRequest\"\n",
    "  },\n",
    "  {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.13; rv:110.0) Gecko/20100101 Firefox/110.0\",\n",
    "    \"X-Requested-With\": \"XMLHttpRequest\"\n",
    "  },\n",
    "  {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36 Edg/110.0.1587.50\",\n",
    "    \"X-Requested-With\": \"XMLHttpRequest\"\n",
    "  },\n",
    "  {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0\",\n",
    "    \"X-Requested-With\": \"XMLHttpRequest\"\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e92df97",
   "metadata": {},
   "source": [
    "### Old: Parallel Webscraping - 20s/movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e435f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "def scrape_movie_details(url):\n",
    "  \"\"\"\n",
    "  Scrape the Movie Details section from the-numbers.com\n",
    "  Returns a dictionary with the structured data\n",
    "  \"\"\"\n",
    "  header = random.choices(HEADERS_LIST)\n",
    "  \n",
    "  s = requests.Session()\n",
    "  r = s.get(url, headers=header, timeout=15)\n",
    "  soup = BeautifulSoup(r.text, 'html.parser')\n",
    "  \n",
    "  # Find the Movie Details section\n",
    "  movie_details = {'link': url}\n",
    "  \n",
    "  # Look for the table with Movie Details\n",
    "  details_table = soup.find('table', {'class': 'movie-details'})\n",
    "  if not details_table:\n",
    "    # Alternative: look for the section by text content\n",
    "    details_section = soup.find('h2', string='Movie Details')\n",
    "    if details_section:\n",
    "      details_table = details_section.find_next('table')\n",
    "  \n",
    "  if details_table:\n",
    "    rows = details_table.find_all('tr')\n",
    "    for row in rows:\n",
    "      cells = row.find_all(['td', 'th'])\n",
    "      if len(cells) >= 2:\n",
    "          key = cells[0].get_text(strip=True).replace('\\xa0', ' ')\n",
    "          value = cells[1].get_text(strip=True).replace('\\xa0', ' ')\n",
    "          \n",
    "          # Clean up the key (remove colons and extra spaces)\n",
    "          key = key.replace(':', '').strip()\n",
    "          \n",
    "          # Skip unwanted fields completely\n",
    "          if key in ['Video Release', 'MPAA Rating', 'Franchise', 'Comparisons']:\n",
    "              continue\n",
    "          \n",
    "          # Handle Production Countries and Languages separately\n",
    "          if key == 'Production Countries':\n",
    "              # Check if Languages data is mixed in\n",
    "              if 'Languages:' in value:\n",
    "                  parts = value.split('Languages:')\n",
    "                  movie_details['Production Countries'] = parts[0].strip()\n",
    "                  if len(parts) > 1:\n",
    "                      movie_details['Languages'] = parts[1].strip()\n",
    "              else:\n",
    "                  movie_details['Production Countries'] = value\n",
    "          elif key == 'Languages':\n",
    "              movie_details['Languages'] = value\n",
    "          else:\n",
    "              # Store all other fields\n",
    "              movie_details[key] = value\n",
    "  \n",
    "  # Extract earliest release date from Domestic and International releases\n",
    "  release_dates = []\n",
    "  \n",
    "  # Extract dates from Domestic Releases\n",
    "  if 'Domestic Releases' in movie_details:\n",
    "      domestic_text = movie_details['Domestic Releases']\n",
    "      # Look for date patterns like \"February 14th, 2025\"\n",
    "      domestic_dates = re.findall(r'([A-Za-z]+ \\d{1,2}(?:st|nd|rd|th)?, \\d{4})', domestic_text)\n",
    "      release_dates.extend(domestic_dates)\n",
    "  \n",
    "  # Extract dates from International Releases\n",
    "  if 'International Releases' in movie_details:\n",
    "      intl_text = movie_details['International Releases']\n",
    "      # Look for date patterns like \"January 29th, 2025\"\n",
    "      intl_dates = re.findall(r'([A-Za-z]+ \\d{1,2}(?:st|nd|rd|th)?, \\d{4})', intl_text)\n",
    "      release_dates.extend(intl_dates)\n",
    "  \n",
    "  # Find the earliest date\n",
    "  if release_dates:\n",
    "      try:\n",
    "          # Convert dates to datetime objects for comparison\n",
    "          parsed_dates = []\n",
    "          for date_str in release_dates:\n",
    "              try:\n",
    "                  # Handle ordinal suffixes (st, nd, rd, th)\n",
    "                  clean_date = re.sub(r'(\\d+)(st|nd|rd|th)', r'\\1', date_str)\n",
    "                  parsed_date = datetime.strptime(clean_date, '%B %d, %Y')\n",
    "                  parsed_dates.append(parsed_date)\n",
    "              except:\n",
    "                  continue\n",
    "          \n",
    "          if parsed_dates:\n",
    "              earliest_date = min(parsed_dates)\n",
    "              movie_details['Release Date'] = earliest_date.strftime('%B %d, %Y')\n",
    "      except:\n",
    "          pass\n",
    "  \n",
    "  # Remove the original release fields since we now have Release Date\n",
    "  movie_details.pop('Domestic Releases', None)\n",
    "  movie_details.pop('International Releases', None)\n",
    "  \n",
    "  return movie_details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5256439f",
   "metadata": {},
   "source": [
    "### New: Async webscraping - 1s/movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b1ecf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from datetime import datetime\n",
    "import random\n",
    "from tqdm.asyncio import tqdm\n",
    "\n",
    "max_concurrency = 10\n",
    "sem = asyncio.Semaphore(max_concurrency)\n",
    "timeout_urls = []\n",
    "\n",
    "async def scrape_movie_details_async(session, header, url):\n",
    "  \"\"\"\n",
    "  Async scraping of movie details.\n",
    "  \"\"\"\n",
    "  async with sem:\n",
    "    try:\n",
    "      async with session.get(url, header=header, timeout=25) as r:\n",
    "        soup = BeautifulSoup(await r.text(), 'html.parser')\n",
    "        # Find the Movie Details section\n",
    "        movie_details = {'link': url}\n",
    "        # Look for the table with Movie Details\n",
    "        details_table = soup.find('table', {'class': 'movie-details'})\n",
    "        if not details_table:\n",
    "          # Alternative: look for the section by text content\n",
    "          details_section = soup.find('h2', string='Movie Details')\n",
    "          if details_section:\n",
    "            details_table = details_section.find_next('table')\n",
    "        \n",
    "        if details_table:\n",
    "          rows = details_table.find_all('tr')\n",
    "          for row in rows:\n",
    "            cells = row.find_all(['td', 'th'])\n",
    "            if len(cells) >= 2:\n",
    "                key = cells[0].get_text(strip=True).replace('\\xa0', ' ')\n",
    "                value = cells[1].get_text(strip=True).replace('\\xa0', ' ')\n",
    "                \n",
    "                # Clean up the key (remove colons and extra spaces)\n",
    "                key = key.replace(':', '').strip()\n",
    "                \n",
    "                # Skip unwanted fields completely\n",
    "                if key in ['Video Release', 'MPAA Rating', 'Franchise', 'Comparisons']:\n",
    "                    continue\n",
    "                \n",
    "                # Handle Production Countries and Languages separately\n",
    "                if key == 'Production Countries':\n",
    "                    # Check if Languages data is mixed in\n",
    "                    if 'Languages:' in value:\n",
    "                        parts = value.split('Languages:')\n",
    "                        movie_details['Production Countries'] = parts[0].strip()\n",
    "                        if len(parts) > 1:\n",
    "                            movie_details['Languages'] = parts[1].strip()\n",
    "                    else:\n",
    "                        movie_details['Production Countries'] = value\n",
    "                elif key == 'Languages':\n",
    "                    movie_details['Languages'] = value\n",
    "                else:\n",
    "                    # Store all other fields\n",
    "                    movie_details[key] = value\n",
    "        \n",
    "        # Extract earliest release date from Domestic and International releases\n",
    "        release_dates = []\n",
    "        \n",
    "        # Extract dates from Domestic Releases\n",
    "        if 'Domestic Releases' in movie_details:\n",
    "            domestic_text = movie_details['Domestic Releases']\n",
    "            # Look for date patterns like \"February 14th, 2025\"\n",
    "            domestic_dates = re.findall(r'([A-Za-z]+ \\d{1,2}(?:st|nd|rd|th)?, \\d{4})', domestic_text)\n",
    "            release_dates.extend(domestic_dates)\n",
    "        \n",
    "        # Extract dates from International Releases\n",
    "        if 'International Releases' in movie_details:\n",
    "            intl_text = movie_details['International Releases']\n",
    "            # Look for date patterns like \"January 29th, 2025\"\n",
    "            intl_dates = re.findall(r'([A-Za-z]+ \\d{1,2}(?:st|nd|rd|th)?, \\d{4})', intl_text)\n",
    "            release_dates.extend(intl_dates)\n",
    "        \n",
    "        # Find the earliest date\n",
    "        if release_dates:\n",
    "            try:\n",
    "                # Convert dates to datetime objects for comparison\n",
    "                parsed_dates = []\n",
    "                for date_str in release_dates:\n",
    "                    try:\n",
    "                        # Handle ordinal suffixes (st, nd, rd, th)\n",
    "                        clean_date = re.sub(r'(\\d+)(st|nd|rd|th)', r'\\1', date_str)\n",
    "                        parsed_date = datetime.strptime(clean_date, '%B %d, %Y')\n",
    "                        parsed_dates.append(parsed_date)\n",
    "                    except:\n",
    "                        continue\n",
    "                \n",
    "                if parsed_dates:\n",
    "                    earliest_date = min(parsed_dates)\n",
    "                    movie_details['Release Date'] = earliest_date.strftime('%B %d, %Y')\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Remove the original release fields since we now have Release Date\n",
    "        movie_details.pop('Domestic Releases', None)\n",
    "        movie_details.pop('International Releases', None)\n",
    "        return movie_details\n",
    "    \n",
    "    except asyncio.TimeoutError:\n",
    "        print(f\"Timeout error for {url}\")\n",
    "        timeout_urls.append(url)\n",
    "        return {}\n",
    "    except Exception as e:\n",
    "        print(f\"Error for {url}: {e}\")\n",
    "        return {}\n",
    "\n",
    "async def scrape_batch(session, header, urls):\n",
    "  \"\"\"Async scraping, continued - callable function.\"\"\"\n",
    "  if session is None:\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "      tasks = [scrape_movie_details_async(session, header, url) for url in urls]\n",
    "      return await tqdm.gather(*tasks)\n",
    "  else:\n",
    "    tasks = [scrape_movie_details_async(session, header, url) for url in urls]\n",
    "    return await tqdm.gather(*tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f392eff9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2800    https://www.the-numbers.com/movie/Kono-sekai-n...\n",
       "2801    https://www.the-numbers.com/movie/Edge-of-Seve...\n",
       "2802    https://www.the-numbers.com/movie/Desu-noto-Li...\n",
       "2803    https://www.the-numbers.com/movie/Asura-The-Ci...\n",
       "2804    https://www.the-numbers.com/movie/Suddenly-Sev...\n",
       "                              ...                        \n",
       "2995    https://www.the-numbers.com/movie/Big-Sick-The...\n",
       "2996    https://www.the-numbers.com/movie/Confidential...\n",
       "2997    https://www.the-numbers.com/movie/Downsizing#t...\n",
       "2998    https://www.the-numbers.com/movie/I-Tonya-(201...\n",
       "2999    https://www.the-numbers.com/movie/Mollys-Game#...\n",
       "Name: link, Length: 200, dtype: object"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Collect all movie details as Series \n",
    "all_movie_details = []\n",
    "urls = [df_WW_all['link'][x:x+200] for x in range(0, len(df_WW_all), 200)]\n",
    "\n",
    "async def process_batch_with_error_handling(session, header, urls, batch_num):\n",
    "    \"\"\"Process one batch with error handling\"\"\"\n",
    "    try:\n",
    "        print(f\"========== PROCESSING BATCH {batch_num:02d} ({len(urls)} URLs)... ==========\")\n",
    "        results = await scrape_batch(urls, session, header)\n",
    "        \n",
    "        # Filter out None results (failed scrapes)\n",
    "        valid_results = [r for r in results if r is not None]\n",
    "        \n",
    "        if valid_results:\n",
    "            # Convert to DataFrame\n",
    "            df = pd.DataFrame(valid_results)\n",
    "            \n",
    "            # Save to CSV\n",
    "            filename = f'Movie Details/movie_details_{batch_num:02d}.csv'\n",
    "            df.to_csv(filename, index=False)\n",
    "            print(f\"✅ Batch {batch_num:02d} completed: {len(valid_results)} movies saved to {filename}\")\n",
    "            return len(valid_results)\n",
    "        else:\n",
    "            print(f\"⚠️ Batch {batch_num:02d} completed but no valid data\")\n",
    "            return 0\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in batch {batch_num:02d}: {e}\")\n",
    "        return 0\n",
    "\n",
    "async def process_all_batches():\n",
    "    \"\"\"Process all batches with error handling\"\"\"\n",
    "    total_processed = 0\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        s = 13 # + 1\n",
    "        batch_num = s\n",
    "        for batch_urls in urls[s:s]:\n",
    "            batch_num += 1\n",
    "            header = random.choices(HEADERS_LIST)\n",
    "            processed_count = await process_batch_with_error_handling(session, header, batch_urls, batch_num)\n",
    "            total_processed += processed_count\n",
    "            \n",
    "            # Small delay between batches to be nice to the server\n",
    "            await asyncio.sleep(1)\n",
    "    \n",
    "    print(f\"\\n🎉 All batches completed! Total movies processed: {total_processed}\")\n",
    "\n",
    "# Run the batch processing\n",
    "await process_all_batches()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af606253",
   "metadata": {},
   "source": [
    "## Concatenate to one dataframe -> Export to csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b6dd4406",
   "metadata": {},
   "outputs": [
    {
     "ename": "EmptyDataError",
     "evalue": "No columns to parse from file",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmptyDataError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mglob\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m all_movie_details \u001b[38;5;241m=\u001b[39m [\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m glob\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMovie Details/*.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Concatenate all series to a dataframe\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m all_movie_details:\n",
      "File \u001b[1;32mc:\\Users\\hithn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hithn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\hithn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hithn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1898\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1895\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1897\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1898\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1899\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1900\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\hithn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:93\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[0;32m     92\u001b[0m     import_optional_dependency(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m \u001b[43mparsers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTextReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39munnamed_cols\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[1;32mparsers.pyx:581\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mEmptyDataError\u001b[0m: No columns to parse from file"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "all_movie_details = [pd.read_csv(file) for file in glob.glob('Movie Details/*.csv')]\n",
    "# Concatenate all series to a dataframe\n",
    "if all_movie_details:\n",
    "  movie_details_df = pd.concat(all_movie_details, axis=0, ignore_index=True)\n",
    "  move_col = movie_details_df.pop('Release Date')\n",
    "  movie_details_df.insert(1,'Release Date', move_col)\n",
    "  print(\"DataFrame shape:\", movie_details_df.shape)\n",
    "  print(\"\\nDataFrame columns:\", movie_details_df.columns.tolist())\n",
    "  print(\"\\n==========WEBSCRAPING COMPLETED==========\\nDataFrame content:\")\n",
    "else:\n",
    "  movie_details_df = pd.DataFrame()\n",
    "\n",
    "movie_details_df.to_csv('Movie Details/movie_details_0000.csv')\n",
    "movie_details_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
