{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2642c6d5",
   "metadata": {},
   "source": [
    "# Webcrawling process (cont') - ƒëang b·ªè d·ªü, c√≥ th·ªÉ kh√¥ng l√†m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2cdee44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dabd49",
   "metadata": {},
   "source": [
    "## Get Movie Details of each film\n",
    "This is the hardest part, not only does it takes time but there is also a risk of being temporarily/permanently blocked by the site (Error 403 Forbidden)..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c9081c",
   "metadata": {},
   "source": [
    "### List of browers to rotate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f95d69a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADERS_LIST = [\n",
    "  {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0\",\n",
    "    \"X-Requested-With\": \"XMLHttpRequest\"\n",
    "  },\n",
    "  {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.13; rv:110.0) Gecko/20100101 Firefox/110.0\",\n",
    "    \"X-Requested-With\": \"XMLHttpRequest\"\n",
    "  },\n",
    "  {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.75 Safari/537.36\",\n",
    "    \"X-Requested-With\": \"XMLHttpRequest\"\n",
    "  },\n",
    "  {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36 Edg/110.0.1587.50\",\n",
    "    \"X-Requested-With\": \"XMLHttpRequest\"\n",
    "  },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e92df97",
   "metadata": {},
   "source": [
    "### Old: Parallel Webscraping - 20s/movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e435f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "def parse_movie_details(soup, url):\n",
    "  \"\"\"\n",
    "  Shared parsing logic for movie details from BeautifulSoup object.\n",
    "  Returns a dictionary with the structured data.\n",
    "  \"\"\"\n",
    "  # Find the Movie Details section\n",
    "  movie_details = {'link': url}\n",
    "  \n",
    "  # Get table #1 (index 1) - the metrics table\n",
    "  production_budget = ''\n",
    "  all_tables = soup.find_all('table', limit=4)\n",
    "  metrics_table = all_tables[1]\n",
    "  \n",
    "  # Search all rows in table #1 for \"Production Budget\"\n",
    "  rows = metrics_table.find_all('tr')\n",
    "  for row in rows:\n",
    "    cells = row.find_all(['td', 'th'])\n",
    "    for cell in (cells):\n",
    "      text = cell.get_text(strip=True)\n",
    "      # Check if this cell contains \"Production Budget\"\n",
    "      if 'production budget' in text.lower():\n",
    "        production_budget = re.search(r'\\$?([\\d,]+)(?![\\d,])', text).group(0)\n",
    "        movie_details['Production Budget'] = production_budget\n",
    "  \n",
    "  # Look for the table with Movie Details\n",
    "  details_table = all_tables[3]\n",
    "  \n",
    "  if details_table:\n",
    "    rows = details_table.find_all('tr')\n",
    "    for row in rows:\n",
    "      cells = row.find_all(['td', 'th'])\n",
    "      if len(cells) >= 2:\n",
    "          key = cells[0].get_text(strip=True).replace('\\xa0', ' ')\n",
    "          value = cells[1].get_text(strip=True).replace('\\xa0', ' ')\n",
    "          \n",
    "          # Clean up the key (remove colons and extra spaces)\n",
    "          key = key.replace(':', '').strip()\n",
    "          \n",
    "          # Skip unwanted fields completely\n",
    "          if key in ['Video Release', 'Comparisons', 'Keywords', 'Source', 'Languages']:\n",
    "              continue\n",
    "          \n",
    "          # MPAA Rating:\n",
    "          if key == 'MPAA Rating':\n",
    "            movie_details['MPAA Rating'] = re.findall(r'^[A-Z0-9-]+', value, flags=re.MULTILINE)\n",
    "\n",
    "          # Handle Production Countries and Languages separately\n",
    "          if key == 'Production Countries':\n",
    "              # Check if Languages data is mixed in\n",
    "              if 'Languages:' in value:\n",
    "                  parts = value.split('Languages:')\n",
    "                  movie_details['Production Countries'] = parts[0].strip()\n",
    "                  if len(parts) > 1:\n",
    "                      movie_details['Languages'] = parts[1].strip()\n",
    "              else:\n",
    "                  movie_details['Production Countries'] = value\n",
    "          elif key == 'Languages':\n",
    "              movie_details['Languages'] = value\n",
    "          else:\n",
    "              # Store all other fields\n",
    "              movie_details[key] = value\n",
    "  \n",
    "  # Extract earliest release date from Domestic and International releases\n",
    "  release_dates = []\n",
    "  \n",
    "  # Extract dates from Domestic Releases\n",
    "  if 'Domestic Releases' in movie_details:\n",
    "      domestic_text = movie_details['Domestic Releases']\n",
    "      # Look for date patterns like \"February 14th, 2025\"\n",
    "      domestic_dates = re.findall(r'([A-Za-z]+ \\d{1,2}(?:st|nd|rd|th)?, \\d{4})', domestic_text)\n",
    "      release_dates.extend(domestic_dates)\n",
    "  \n",
    "  # Extract dates from International Releases\n",
    "  if 'International Releases' in movie_details:\n",
    "      intl_text = movie_details['International Releases']\n",
    "      # Look for date patterns like \"January 29th, 2025\"\n",
    "      intl_dates = re.findall(r'([A-Za-z]+ \\d{1,2}(?:st|nd|rd|th)?, \\d{4})', intl_text)\n",
    "      release_dates.extend(intl_dates)\n",
    "  \n",
    "  # Find the earliest date\n",
    "  if release_dates:\n",
    "      try:\n",
    "          # Convert dates to datetime objects for comparison\n",
    "          parsed_dates = []\n",
    "          for date_str in release_dates:\n",
    "              try:\n",
    "                  # Handle ordinal suffixes (st, nd, rd, th)\n",
    "                  clean_date = re.sub(r'(\\d+)(st|nd|rd|th)', r'\\1', date_str)\n",
    "                  parsed_date = datetime.strptime(clean_date, '%B %d, %Y')\n",
    "                  parsed_dates.append(parsed_date)\n",
    "              except:\n",
    "                  continue\n",
    "          \n",
    "          if parsed_dates:\n",
    "              earliest_date = min(parsed_dates)\n",
    "              movie_details['Release Date'] = earliest_date.strftime('%B %d, %Y')\n",
    "      except:\n",
    "          pass\n",
    "  \n",
    "  # Remove the original release fields since we now have Release Date\n",
    "  movie_details.pop('Domestic Releases', None)\n",
    "  movie_details.pop('International Releases', None)\n",
    "  \n",
    "  return movie_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c993c4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_movie_details(url):\n",
    "  \"\"\"\n",
    "  Scrape the Movie Details section from the-numbers.com\n",
    "  Returns a dictionary with the structured data\n",
    "  \"\"\"\n",
    "  header = random.choice(HEADERS_LIST)  # Use choice() not choices()\n",
    "  \n",
    "  s = requests.Session()\n",
    "  r = s.get(url, headers=header, timeout=15)\n",
    "  soup = BeautifulSoup(r.text, 'html.parser')\n",
    "  \n",
    "  # Use the shared parsing logic\n",
    "  return parse_movie_details(soup, url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5256439f",
   "metadata": {},
   "source": [
    "### New: Async webscraping - 1s/movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0b1ecf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm.asyncio import tqdm\n",
    "\n",
    "max_concurrency = 10\n",
    "sem = asyncio.Semaphore(max_concurrency)\n",
    "timeout_urls = []\n",
    "\n",
    "async def scrape_movie_details_async(session, header, url):\n",
    "  \"\"\"\n",
    "  Async scraping of movie details.\n",
    "  \"\"\"\n",
    "  async with sem:\n",
    "    try:\n",
    "      async with session.get(url, headers=header, timeout=25) as r:\n",
    "        status = r.status\n",
    "        if status != 200:\n",
    "          print(f\"HTTP {status} for {url}\")\n",
    "          return None\n",
    "        html = await r.text()\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        \n",
    "        # Use the shared parsing logic\n",
    "        data = parse_movie_details(soup, url)\n",
    "        # Treat dicts with only 'link' as invalid\n",
    "        if isinstance(data, dict) and len(data) <= 1:\n",
    "          print(f\"Parsed no data for {url}\")\n",
    "          return None\n",
    "        return data\n",
    "    \n",
    "    except asyncio.TimeoutError:\n",
    "        print(f\"Timeout error for {url}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error for {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "async def scrape_batch(session, header, urls):\n",
    "  \"\"\"Async scraping, continued - callable function.\"\"\"\n",
    "  if session is None:\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "      tasks = [scrape_movie_details_async(session, header, url) for url in urls]\n",
    "      return await tqdm.gather(*tasks)\n",
    "  else:\n",
    "    tasks = [scrape_movie_details_async(session, header, url) for url in urls]\n",
    "    return await tqdm.gather(*tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f392eff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def process_batch_with_error_handling(session, header, urls, batch_num):\n",
    "  \"\"\"Process one batch with error handling\"\"\"\n",
    "  try:\n",
    "    print(f\"========== PROCESSING BATCH {batch_num:02d} ({len(urls)} URLs)... ==========\")\n",
    "\n",
    "    results = await scrape_batch(session, header, urls)\n",
    "    \n",
    "    # Keep only non-empty dicts with more than just the link\n",
    "    valid_results = []\n",
    "    for r in results:\n",
    "      if isinstance(r, dict) and len(r) > 1:\n",
    "        valid_results.append(r)\n",
    "    \n",
    "    if valid_results:\n",
    "      # Convert to DataFrame\n",
    "      df = pd.DataFrame(valid_results)\n",
    "      \n",
    "      # Save to CSV\n",
    "      filename = f'Movie Details/movie_details_{batch_num:02d}.csv'\n",
    "      df.to_csv(filename, index=False)\n",
    "      print(f\"‚úÖ Batch {batch_num:02d} completed: {len(valid_results)} movies saved to {filename}\")\n",
    "      return len(valid_results)\n",
    "    else:\n",
    "      print(f\"‚ö†Ô∏è Batch {batch_num:02d} completed but no valid data\")\n",
    "      return None\n",
    "            \n",
    "  except Exception as e:\n",
    "    print(f\"‚ùå Error in batch {batch_num:02d}: {e}\")\n",
    "    return None\n",
    "\n",
    "async def process_all_batches(urls, start_at=0):\n",
    "  \"\"\"Process all batches with error handling.\n",
    "  Accepts:\n",
    "    - string URL ‚Üí one batch with one URL\n",
    "    - list[str]  ‚Üí one batch with many URLs\n",
    "    - list[list[str]] ‚Üí multiple batches (original behavior)\n",
    "  \"\"\"\n",
    "  # Normalize input into list of batches (list[list[str]])\n",
    "  if isinstance(urls, str):\n",
    "    batches = [[urls]]\n",
    "  elif isinstance(urls, list):\n",
    "    if len(urls) == 0:\n",
    "      batches = []\n",
    "    elif all(isinstance(u, str) for u in urls):\n",
    "      batches = [urls]\n",
    "    else:\n",
    "      batches = urls\n",
    "  else:\n",
    "    batches = []\n",
    "  \n",
    "  # Clamp start_at\n",
    "  if start_at is None or not isinstance(start_at, int):\n",
    "    start_at = 0\n",
    "  if start_at < 0:\n",
    "    start_at = 0\n",
    "  if 90 > start_at >= len(batches) > 0:\n",
    "    start_at = len(batches) - 1\n",
    "  if start_at == 90:\n",
    "    start_at = 90\n",
    "  \n",
    "  total_processed = 0\n",
    "\n",
    "  async with aiohttp.ClientSession() as session:\n",
    "    batch_num = start_at\n",
    "    for batch_urls in batches:\n",
    "      batch_num += 1\n",
    "      i = (batch_num - 1) // 2 % len(HEADERS_LIST)\n",
    "      header = HEADERS_LIST[i]\n",
    "      processed_count = await process_batch_with_error_handling(session, header, batch_urls, batch_num)\n",
    "      total_processed += processed_count\n",
    "      \n",
    "      # Small delay between batches to be nice to the server\n",
    "      await asyncio.sleep(1)\n",
    "  \n",
    "  print(f\"\\nüéâ All batches completed! Total movies processed: {total_processed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b560dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_WW_all = pd.read_csv(\"WW_all.csv\")\n",
    "all_movie_details = []\n",
    "links = [df_WW_all['link'].tolist()[x:x+200] for x in range(0, len(df_WW_all), 200)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f815ba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links_to_scrape = links\n",
    "isinstance(links_to_scrape, list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "674d74b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape_movie_details(links_to_scrape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf7e85f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== PROCESSING BATCH 01 (200 URLs)... ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:51<00:00,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Batch 01 completed: 200 movies saved to Movie Details/movie_details_01.csv\n",
      "========== PROCESSING BATCH 02 (200 URLs)... ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:47<00:00,  4.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Batch 02 completed: 200 movies saved to Movie Details/movie_details_02.csv\n",
      "========== PROCESSING BATCH 03 (200 URLs)... ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:44<00:00,  4.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Batch 03 completed: 200 movies saved to Movie Details/movie_details_03.csv\n",
      "========== PROCESSING BATCH 04 (200 URLs)... ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:51<00:00,  3.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Batch 04 completed: 200 movies saved to Movie Details/movie_details_04.csv\n",
      "========== PROCESSING BATCH 05 (200 URLs)... ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:59<00:00,  3.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Batch 05 completed: 200 movies saved to Movie Details/movie_details_05.csv\n",
      "========== PROCESSING BATCH 06 (200 URLs)... ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:49<00:00,  4.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Batch 06 completed: 200 movies saved to Movie Details/movie_details_06.csv\n",
      "========== PROCESSING BATCH 07 (200 URLs)... ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:49<00:00,  4.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Batch 07 completed: 200 movies saved to Movie Details/movie_details_07.csv\n",
      "========== PROCESSING BATCH 08 (200 URLs)... ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:44<00:00,  4.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Batch 08 completed: 200 movies saved to Movie Details/movie_details_08.csv\n",
      "========== PROCESSING BATCH 09 (200 URLs)... ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:36<00:00,  5.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Batch 09 completed: 200 movies saved to Movie Details/movie_details_09.csv\n",
      "========== PROCESSING BATCH 10 (200 URLs)... ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:50<00:00,  3.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Batch 10 completed: 200 movies saved to Movie Details/movie_details_10.csv\n",
      "========== PROCESSING BATCH 11 (200 URLs)... ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:45<00:00,  4.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Batch 11 completed: 200 movies saved to Movie Details/movie_details_11.csv\n",
      "========== PROCESSING BATCH 12 (200 URLs)... ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:49<00:00,  4.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Batch 12 completed: 200 movies saved to Movie Details/movie_details_12.csv\n",
      "========== PROCESSING BATCH 13 (200 URLs)... ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:39<00:00,  5.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Batch 13 completed: 200 movies saved to Movie Details/movie_details_13.csv\n",
      "========== PROCESSING BATCH 14 (200 URLs)... ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:43<00:00,  4.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Batch 14 completed: 200 movies saved to Movie Details/movie_details_14.csv\n",
      "========== PROCESSING BATCH 15 (200 URLs)... ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:44<00:00,  4.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Batch 15 completed: 200 movies saved to Movie Details/movie_details_15.csv\n",
      "========== PROCESSING BATCH 16 (200 URLs)... ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:30<00:00,  6.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Batch 16 completed: 200 movies saved to Movie Details/movie_details_16.csv\n",
      "========== PROCESSING BATCH 17 (200 URLs)... ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:56<00:00,  3.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Batch 17 completed: 200 movies saved to Movie Details/movie_details_17.csv\n",
      "========== PROCESSING BATCH 18 (200 URLs)... ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:41<00:00,  4.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Batch 18 completed: 200 movies saved to Movie Details/movie_details_18.csv\n",
      "========== PROCESSING BATCH 19 (200 URLs)... ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:37<00:00,  5.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Batch 19 completed: 200 movies saved to Movie Details/movie_details_19.csv\n",
      "========== PROCESSING BATCH 20 (200 URLs)... ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:37<00:00,  5.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Batch 20 completed: 200 movies saved to Movie Details/movie_details_20.csv\n",
      "========== PROCESSING BATCH 21 (200 URLs)... ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|‚ñà‚ñà‚ñà‚ñå      | 72/200 [00:12<00:14,  8.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP 403 for https://www.the-numbers.com/movie/Northman-The#tab=summary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:33<00:00,  5.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Batch 21 completed: 199 movies saved to Movie Details/movie_details_21.csv\n",
      "========== PROCESSING BATCH 22 (200 URLs)... ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 171/200 [00:58<01:01,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for https://www.the-numbers.com/movie/Ce-ancora-domani-(2023-Italy)#tab=summary: list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 175/200 [00:59<00:32,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timeout error for https://www.the-numbers.com/movie/She-Said-(2022)#tab=summary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [01:14<00:00,  2.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Batch 22 completed: 198 movies saved to Movie Details/movie_details_22.csv\n",
      "========== PROCESSING BATCH 23 (200 URLs)... ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:37<00:00,  5.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Batch 23 completed: 200 movies saved to Movie Details/movie_details_23.csv\n",
      "========== PROCESSING BATCH 24 (200 URLs)... ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:37<00:00,  5.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Batch 24 completed: 200 movies saved to Movie Details/movie_details_24.csv\n",
      "========== PROCESSING BATCH 25 (40 URLs)... ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 40/40 [00:05<00:00,  7.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Batch 25 completed: 40 movies saved to Movie Details/movie_details_25.csv\n",
      "\n",
      "üéâ All batches completed! Total movies processed: 4837\n"
     ]
    }
   ],
   "source": [
    "await process_all_batches(urls=links_to_scrape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af606253",
   "metadata": {},
   "source": [
    "## Concatenate to one dataframe & Export to csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6dd4406",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "def export_all_movie_details():\n",
    "  all_movie_details = [pd.read_csv(file) for file in glob.glob('Movie Details/*.csv')]\n",
    "  global movie_details_df\n",
    "  # Concatenate all series to a dataframe\n",
    "  if all_movie_details:\n",
    "    movie_details_df = pd.concat(all_movie_details, axis=0, ignore_index=True)\n",
    "    move_col = movie_details_df.pop('Release Date')\n",
    "    movie_details_df.insert(1,'Release Date', move_col)\n",
    "    \n",
    "    print(\"DataFrame shape:\", movie_details_df.shape)\n",
    "    print(\"\\nDataFrame columns:\", movie_details_df.columns.tolist())\n",
    "    print(\"\\nDataFrame content:\")\n",
    "  else:\n",
    "    movie_details_df = pd.DataFrame()\n",
    "\n",
    "  movie_details_df.to_csv('movie_details.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0b8d847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame shape: (5158, 12)\n",
      "\n",
      "DataFrame columns: ['link', 'Release Date', 'Production Budget', 'MPAA Rating', 'Running Time', 'Franchise', 'Genre', 'Production Method', 'Creative Type', 'Production/Financing Companies', 'Production Countries', 'Languages']\n",
      "\n",
      "DataFrame content:\n"
     ]
    }
   ],
   "source": [
    "export_all_movie_details()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47136cd5",
   "metadata": {},
   "source": [
    "### Re-run timeouts & append to final dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e7f524f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timeouts: 1 missing movies.\n",
      "Retrying timed out URLs...\n",
      "========== PROCESSING BATCH 92 (1 URLs)... ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP 403 for https://www.the-numbers.com/movie/Northman-The#tab=summary\n",
      "‚ö†Ô∏è Batch 92 completed but no valid data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +=: 'int' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m     export_all_movie_details()\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m retry_timeouts(retry_start \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m retry_timeouts()\n",
      "Cell \u001b[1;32mIn[14], line 14\u001b[0m, in \u001b[0;36mretry_timeouts\u001b[1;34m(retry_start)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTimeouts: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimeouts_df\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m missing movies.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying timed out URLs...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m process_all_batches(urls\u001b[38;5;241m=\u001b[39mtimeout_urls, start_at\u001b[38;5;241m=\u001b[39mretry_start)\n\u001b[0;32m     15\u001b[0m export_all_movie_details()\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m retry_timeouts(retry_start \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[6], line 70\u001b[0m, in \u001b[0;36mprocess_all_batches\u001b[1;34m(urls, start_at)\u001b[0m\n\u001b[0;32m     68\u001b[0m header \u001b[38;5;241m=\u001b[39m HEADERS_LIST[i]\n\u001b[0;32m     69\u001b[0m processed_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m process_batch_with_error_handling(session, header, batch_urls, batch_num)\n\u001b[1;32m---> 70\u001b[0m \u001b[43mtotal_processed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mprocessed_count\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# Small delay between batches to be nice to the server\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for +=: 'int' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_WW_all = pd.read_csv(\"WW_all.csv\")\n",
    "movie_details_df = pd.read_csv(\"movie_details.csv\").dropna(subset=['link'])\n",
    "# Find differences\n",
    "\n",
    "async def retry_timeouts(retry_start=91):\n",
    "  timeouts_df = df_WW_all[~df_WW_all['link'].isin(movie_details_df['link'])]\n",
    "  timeout_urls = [timeouts_df['link'].tolist()[x:x+400] for x in range(0, len(timeouts_df), 400)]\n",
    "  if not timeout_urls:\n",
    "    print(\"No more timeouts :D\")\n",
    "  else:\n",
    "    print(f\"Timeouts: {timeouts_df.shape[0]} missing movies.\")\n",
    "    print(f\"Retrying timed out URLs...\")\n",
    "    await process_all_batches(urls=timeout_urls, start_at=retry_start)\n",
    "    export_all_movie_details()\n",
    "    await retry_timeouts(retry_start + 1)\n",
    "\n",
    "await retry_timeouts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd3738c",
   "metadata": {},
   "source": [
    "## Merge to final `df_WW_all`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7870e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 5306 entries, 0 to 5303\n",
      "Data columns (total 17 columns):\n",
      " #   Column                    Non-Null Count  Dtype \n",
      "---  ------                    --------------  ----- \n",
      " 0   Year Recorded             5306 non-null   int64 \n",
      " 1   Rank                      5306 non-null   int64 \n",
      " 2   Movie                     5306 non-null   object\n",
      " 3   Worldwide Box Office      5306 non-null   object\n",
      " 4   Domestic Box Office       3826 non-null   object\n",
      " 5   International Box Office  5246 non-null   object\n",
      " 6   Domestic Share            3826 non-null   object\n",
      " 7   Distributor               3686 non-null   object\n",
      " 8   Production Budget         2930 non-null   object\n",
      " 9   Running Time              4876 non-null   object\n",
      " 10  Genre                     5218 non-null   object\n",
      " 11  Production Method         5204 non-null   object\n",
      " 12  Creative Type             4802 non-null   object\n",
      " 13  MPAA Rating               3710 non-null   object\n",
      " 14  Franchise                 1294 non-null   object\n",
      " 15  Production Countries      5251 non-null   object\n",
      " 16  Release Date              5145 non-null   object\n",
      "dtypes: int64(2), object(15)\n",
      "memory usage: 746.2+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "df_WW_all = pd.read_csv(\"WW_all.csv\")\n",
    "movie_details_df = pd.read_csv(\"movie_details.csv\").dropna(subset=['link'])\n",
    "# MERGE\n",
    "df = pd.merge(df_WW_all, movie_details_df, on='link', how='right').sort_values(by=['Year Recorded','Rank'])\n",
    "\n",
    "# CLEAN MPAA RATING - extract only the rating (PG, PG-13, R, G, etc.)\n",
    "if 'MPAA Rating' in df.columns:\n",
    "    df['MPAA Rating'] = df['MPAA Rating'].apply(\n",
    "        lambda x: re.match(r'^([A-Z]+(?:-[0-9]+)?)', str(x)).group(1) \n",
    "        if pd.notna(x) and isinstance(x, str) and re.match(r'^([A-Z]+(?:-[0-9]+)?)', str(x)) \n",
    "        else x\n",
    "    )\n",
    "\n",
    "# COMBINE 2 COLUMNS\n",
    "df[\"Distributor_y\"] = np.where(\n",
    "    df[\"Distributor\"].isna() | (df[\"Distributor\"] == \"\"),  # A2 = \"\"\n",
    "    df[\"Production/Financing Companies\"].apply(\n",
    "        lambda x: x.split(\",\")[0].strip() if isinstance(x, str) and \",\" in x else np.nan\n",
    "    ),\n",
    "    np.nan\n",
    ")\n",
    "df[\"Distributor_y\"] = df[\"Distributor_y\"].fillna(df[\"Distributor\"])\n",
    "df = df.drop(columns=['Distributor'])\n",
    "\n",
    "# RENAME\n",
    "df = df.rename(columns={'Distributor_y': 'Distributor',\n",
    "                      'Genre_y': 'Genre',\n",
    "                      'Release Date_y': 'Release Date'})\n",
    "\n",
    "# REORDER\n",
    "df = df[['Year Recorded', 'Rank', 'Movie', 'Worldwide Box Office', 'Domestic Box Office', 'International Box Office', 'Domestic Share', 'Distributor', 'Production Budget', 'Running Time', 'Genre', 'Production Method', 'Creative Type', 'MPAA Rating', 'Franchise', 'Production Countries', 'Release Date']]\n",
    "df.to_csv('WW_all_new.csv', index=False)\n",
    "\n",
    "df.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
